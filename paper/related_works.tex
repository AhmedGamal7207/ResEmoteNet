\section{Related Works}

\subsection{Convolutional Neural Network}

Convolutional Neural Network (CNN) is a type of deep learning model utilized for processing grid-like data such as images. It utilizes convolutional layers to automatically recognize patterns or characteristics through spatial hierarchies, allowing the network to grasp increasingly intricate data, and pooling layers to decrease dimensionality and computational complexity, while preserving crucial features. These networks have shown notable success in tasks related to computer vision like image classification, segmentation, and object detection. The primary benefit of CNNs is their capacity to automatically learn intricate features without requiring manual feature engineering. Additionally, CNNs have great flexibility in accommodating various input sizes and are effective in processing intricate patterns and data fluctuations.

The first ever CNN introduced in 1998 and was called LeNet-5 \cite{lecun1998gradient}. It was used for handwritten digit recognition problem, which laid the foundation for modern CNN architecture. But the CNN architecture has then gained its popularity with the introduction of AlexNet \cite{krizhevsky2012imagenet} in the 2012 ImageNet Large Scale Visual Recognition Challenge (ILSVRC). Since then, novel CNN architectures like GoogLeNet \cite{szegedy2015going}, ResNet \cite{he2016deep}, InceptionV3 \cite{szegedy2016rethinking}, VGG \cite{simonyan2014very}, MobileNet \cite{howard2017mobilenets}, and more have been proposed for the general image classification task.

Recently, there has been a shift towards utilizing deep learning solutions for facial expression recognition due to advancements in GPU technology and the availability of established deep learning libraries. These solutions frequently surpass the manually created techniques, particularly in databases found in natural environments. Although there are numerous deep learning approaches for FER, CNN has continued to be the preferred option for previous studies.

\subsection{Squeeze and Excitation Network}

Squeeze-and-Excitation Block (SE) is an architectural unit introduced in 2018 by Sun \cite{hu2018squeeze} was designed to improve the representational power of a network by enabling it to perform dynamic channel-wise recalibration. CNNs have shown their usefulness in addressing various visual tasks \cite{krizhevsky2012imagenet}, \cite{toshev2014deeppose}, \cite{long2015fully}, \cite{ren2015faster}. In each convolutional layer of the network, a group of filters showcases local spatial patterns connecting neighbouring input channels, blending spatial and channel-specific data within small receptive fields. CNNs achieve global theoretical receptive fields by combining convolutional layers with activation functions and downsampling operators, capturing hierarchical patterns in image representations. One main focus of computer vision studies is finding stronger representations that highlight the most important aspects of an image for a specific task, leading to better outcomes.

The creation and development of new CNN architectures is a demanding engineering effort that often necessitates the selection of numerous additional hyper-parameters and layer combinations. The SE block, on the other hand, has a basic structure and may be employed directly in existing cutting-edge designs by replacing components with their SE counterparts, resulting in improved performance. SE blocks are likewise computationally light, leading to just a minor increase in model complexity and computational strain.

\subsection{Explainable AI}
Explainable AI (XAI) is sometimes referred to as interpretable AI, and it involves having humans able to maintain intellectual control over an artificial intelligence system, or the techniques used to make this possible. The primary emphasis is usually on the rationale behind the decisions or predictions made by the AI, which are enhanced for better comprehension and transparency. XAI addresses the issue of machine learning being a "black box," in which the creators of the AI are unable to provide an explanation for its decision-making process.

For many years, the development of computer vision has been greatly influenced by the goal of comprehending the visual world with the help of machines \cite{juneja2013blocks, mahendran2016visualizing, parikh2011human, singh2012unsupervised, sudderth2005learning}. Initial efforts to understand recognition models involved visualizing filter responses, reconstructing inputs from network layers, and creating inputs to activate specific neurons. Lately, advancements like Class Activation Mapping (CAM) \cite{zhou2016learning}, Gradient-weighted CAM (Grad-CAM) \cite{selvaraju2017grad}, and Grad-CAM++ \cite{chattopadhay2018grad}, have expanded the limits. CAM uses global pooling to identify specific regions of interest, whereas Grad-CAM provides a broader approach to visualize all convolutional filters, uncovering hidden insights in neural networks. The search for significant representations in facial recognition stretches back to an earlier time \cite{chen2002principle, learned2016labeled, o2018face}. Traditional methods typically emphasized precision rather than ease of understanding, constructing models based on facial parts or characteristics. Although these techniques made advancements, they did not have clear transparency in their collection and use of facial data, closing the divide in the area of understandable representation learning.

